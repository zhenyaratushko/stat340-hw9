---
title: "Homework 9"
output: html_document
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
```


## Problem 1 More regression with `mtcars` (12 points; 2 pts each)

In lecture, we worked briefly with the `mtcars` data set.
Let's get more regression practice by working with it some more.

### a) background

Run `?mtcars` in the console (please __do not__ add it to this `Rmd` file) and briefly read the help page.
Specifically, take note of the following:

1. What is the source of this data? 
2. What is this data set measuring (i.e., what was the response variable in the original study, at least based on the brief description in the R documentation)?
3. What predictors are available and what do they mean?

***

1. This data comes from the 1974 Motor Trend US magazine.
2. This data is measuring the fuel consumption (mpg) of 32 automobiles (1973â€“74 models).
3. There are ten predictors available that all comprise other aspects of automobile design and performance, including number of cylinders, displacement, gross horsepower, rear axle ratio, weight, 1/4 mile time, engine, transmission, number of forward gears, and number of carburetors.

***

You may want to also run `head(mtcars, 10)` or `View(mtcars)` to inspect the data frame briefly before moving on.

### b) Fitting a model

Use `lm` to run a regression of `mpg` on a few predictors in the data frame (choose two or three that you think would make a good model-- don't use all ten; we'll talk about why in later lectures).
Make sure to include `data = mtcars` as a keyword argument to `lm` so that R knows what data frame to use.

```{r}
lm.mtcars = lm(mpg ~ 1 + hp + wt + vs, data = mtcars)
plot(lm.mtcars, ask = F, which = 1:2)
```

Briefly inspect the residuals plot by running `plot(lm.mtcars,ask=F,which=1:2)`.
What do you observe, and what does it mean?

***

I observe a non-linear trend in the data, which means that it must not align with the chosen model, which IS linear. To make it more accurate, we may need to add some squared terms.

***

### c) Interpreting the model

View the summary of your model by uncommenting and running the code below.

```{r}
summary(lm.mtcars)
```

Pick one of your predictors and give an interpretation of the estimate and standard error for its coefficient.
Be careful in your wording of the interpretation.

***

My chosen predictor is weight. On average, for every one unit increase in weight, the mpg will change by the estimate -3.78003 with a standard error of plus or minus 0.63985 for its coefficient.

***

Which coefficients are statistically significantly different from zero? How do you know?

***

Gross horsepower and weight are statistically significantly different from zero because both of their p-values fall below the 0.05 threshold.

***

### d) Interpreting residuals

What is the Residual Standard Error (RSE) for this model? How many degrees of freedom does it have?

***

The residual standard error for this model is 2.592 on 28 degrees of freedom.

***

What is the value of $R^2$ for this model? (__Hint:__ look at the output of `summary`) Give an interpretation of this value.

***

The multiple R-squared value for this model is 0.8329. This value shows how well a regression model (independent variable) predicts the outcome of observed data (dependent variable), falling on a range from 0 to 1 with 1 being the best. It can be interpreted as those chosen predictors being able to explain that much of the variance of mpg (i.e. goodness of fit).

***

### e) Adjusted $R^2$

Briefly read about the adjusted $R^2$ [here](https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/adjusted-r2/).
What is the adjusted $R^2$ of this model and how does this differ from the usual $R^2$ value? (__Hint:__ again, look at the output of `summary`).

***

The adjusted R-squared value for this model is 0.815. This adjusted value shows how well terms (data points) fit a curve or line, and it adjusts for the number of terms in a model (i.e. adding more useless variables to a model will decrease the adjusted r-squared value and adding more useful ones will increase the value). The adjusted R-squared value will always be less than or equal to the regular R-squared value.

***

### f) CIs for coefficients

Read the documentation for the `confint` function, and use it to generate $95\%$ confidence intervals for the coefficients of your model.
Give an interpretation of these confidence intervals.

```{r}
confint(lm.mtcars, level = 0.95)
```

***

The true value of my model's coefficients will fall in the above intervals with 95% confidence.

***


## Problem 2) the `cats` data set (8 points; 2pts each)

The `cats` data set, included in the `MASS` library, contains data recorded from 144 cats.
Each row of the data set contains the body weight (`Bwt`, in kgs), heart weight (`Hwt`, in grams) and the sex (`Sex`, levels `'F'` and `'M'`) for one of the cats in the data set.

### a) plotting the data

Create a scatter plot showing heart weight on the y-axis and body weight on the x-axis.
Ignore the `Sex` variable in this plot.

```{r}
library(MASS)
head(cats)
```

```{r}
ggplot(cats, aes(x = Bwt, y = Hwt)) + 
    geom_point()
```

Briefly describe what you see. Is there a clear trend in the data?

There does seem to be a positive linear trend in the data.

### b) fitting a linear model

Fit a linear regression model to predict cat heart weight from cat body weight (and using an intercept term, of course).

```{r}
weight_lm = lm(Hwt ~ 1 + Bwt, data = cats)
summary(weight_lm)
```

Examine the coefficients of your fitted model.
What is the coefficient for the `Bwt` variable?
Interpret this coefficient-- a unit change in body weight yields how much change in heart weight?

***

The coefficient for the `Bwt` variable is 4.0341, which means that a unit change in body weight will yield that much change in heart weight.

***

### c) back to plotting

Create the same plot from Part a above, but this time color the points in the scatter plot according to the `Sex` variable.
You may use either `ggplot2` or the built-in R plotting tools, though I would recommend the former, for this.

You should see a clear pattern. Describe it. A sentence or two is fine here.

```{r}
ggplot(cats, aes(x = Bwt, y = Hwt, color = Sex)) + 
    geom_point()
```

***

In this graph, females are clustered at smaller average heart and body weights, whereas males are more spread out.

***

### d) adding `Sex` and an interaction

From looking at the data, it should be clear that the `Sex` variable has explanatory power in predicting heart weight, but it is also very correlated with body weight.

Fit a new linear regression model, still predicting heart weight, but this time including both body weight and sex as predictors *and* an interaction term between body weight and sex.
Take note of how R assigns `Sex` a dummy encoding.

```{r}
weight_v2_lm = lm(Hwt ~ 1 + Bwt + Sex + Bwt:Sex, data = cats) 
summary(weight_v2_lm)
```

Examine the outputs of your model.
In particular, note the coefficients of `Sex` and the interaction between `Bwt` and `Sex`.
Are both of these coefficients statistically significantly different from zero?
How do you interpret the interaction term?

***

Yes, both of these coefficients are statistically significantly different from zero because their p-values are less than 0.05. The interaction term shows that the effect of body weight changes by 1.6763 when the sex is M instead of F (i.e. shows the unique effect of sex on body weight).

***


## Problem 3 - Using Multiple regression to fit nonlinear data (10 points, 2.5 pts each)

Open the dataset `multData.csv`. This data set consists of three predictor variables, simply named `X1`, `X2` and `X3`. The response variable is `Y`. In this problem you will explore how to use the multiple regression model to model nonlinear relationships.

### a) the first model

First we will explore the relationship between $Y$ and the first two predictors $X1$ and $X2$. Fit the linear model

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon$$
Interpret the coefficients of both X1 and X2. 

```{r}
multiple_data = read_csv("multData.csv")
```

```{r}
bo = lm(Y ~ 1 + X1 + X2, data = multiple_data)
summary(bo)
```

***

X1 = -6.7573
X2 = -22.8693

On average, every unit change in Y will lead to the above amount of change in X1 and X2.

***


### b) Investigating interaction of quantitative predictors

Next introduce an interaction term to the model
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2 + \epsilon$$

Fit the model and view the summary output. Has this improved the model fit? Did anything surprising happen to the coefficients? Try to explain what happened.


```{r}
bo_v1 = lm(Y ~ 1 + X1 + X2 + X1:X2, data = multiple_data)
summary(bo_v1)
```

***

This has improved the model fit, with the adjusted R-squared value increasing to 0.9816. The significance of X2 has changed from model to model, being significant in our first model and not so much in the second one, showing how the interaction term is able to capture the majority of X2's effect on Y.

***


### c) Introducing the last predictor

Next fit a model that introduces the `X3` variable. 

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1\cdot X_2  + \beta_4 X_3 \epsilon$$
Has the model fit improved? In what way (Justify your answer)? 

```{r}
bo_v2 = lm(Y ~ 1 + X1 + X2 + X1:X2 + X3, data = multiple_data)
summary(bo_v2)
```

***

Yes, the model fit has improved, because the multiple R-squared value rose to 0.9994 and the adjusted to 0.9993.

***


### d) Considering higher order terms

Finally explore higher order terms for the X3 variable: Introduce $X3^2$, $X3^3$ etc and determine if any of these higher order terms are justified in the model. Explain your reasoning and present your final model. Look at the diagnostic plots and discuss whether the assumptions of the multiple regression model seem to be justified.

```{r}
bo_v3 = lm(Y ~ 1 + X1 + X2 + X1:X2 + X3 + I(X3^3), data = multiple_data)
summary(bo_v3)
```

```{r}
plot(bo_v3)
```


***

None of the higher order terms are justified in the model because the r-squared values do not change at all, no matter how different the powers are. The assumptions of the multiple regression model DO seem to be justified, because this model is linear (mean of residuals is 0), homoscedastic (has constant variance), and normal (points follow Q-Q line).

***
